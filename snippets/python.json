{

	// import

	"import default packages":{
		"prefix":"import default",
		"body":[
			"import argparse",
			"import datetime",
			"import glob",
			"import json",
			"import os",
			"import pathlib",
			"import sys",
			"import traceback",
			"",
			"import matplotlib.pyplot as plt",
			"import numpy as np",
			"import pandas as pd",
			"from tqdm import tqdm",
			""
		]
	},
	"import scipy.stats": {
		"prefix": "import scipy.stats",
		"body": [
			"from scipy import stats"
		]
	},
	"import scipy.special": {
		"prefix": "import scipy.special",
		"body": [
			"from scipy import special"
		]
	},
	"import sklearn.metrics": {
		"prefix": "import sklearn.metrics",
		"body": [
			"from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score"
		]
	},
	"import sklearn.iris_dataset": {
		"prefix": "import sklearn iris-dataset",
		"body": [
			"from sklearn.datasets import load_iris"
		]
	},
	"import sklearn.tree": {
		"prefix": "import sklearn RandomForestRegressor",
		"body": [
			"from sklearn import tree"
		]
	},
	"import sklearn.ensemble": {
		"prefix": "import sklearn ensemble",
		"body": [
			"from sklearn import tree",
			"from sklearn import ensemble"
		]
	},
	"import sklearn.KMeans": {
		"prefix": "import sklearn KMeans",
		"body": [
			"from sklearn.cluster import KMeans"
		]
	},
	"import pytorch-lightning": {
		"prefix": "import pytorch-lightning",
		"body": [
			"import torch",
			"import torch.optim as optim",
			"from torch import nn",
			"from torch.nn import functional as F",
			"from torch.utils.data import DataLoader, Dataset",
			"import torchvision.transforms as T",
			"from torchvision import io",
			"import pytorch_lightning as pl",
			"from pytorch_lightning import LightningModule, LightningDataModule, callbacks",
			"from pytorch_lightning.loggers import MLFlowLogger",
			"from sklearn import model_selection"
		]
	},
	"import lightgbm": {
		"prefix": "import lightgbm",
		"body": [
			"from sklearn.model_selection import train_test_split",
			"import lightgbm as lgb"
		]
	},
	"import sqlalchemy": {
		"prefix": "import sqlalchemy",
		"body": [
			"import sqlalchemy",
			"from sqlalchemy.engine.base import Engine",
			"from sqlalchemy.orm import declarative_base, sessionmaker, DeclarativeMeta, Mapped",
			"from sqlalchemy.orm.session import Session",
			"from sqlalchemy.schema import Column",
			"from sqlalchemy.types import Boolean, Date, Integer, String",
			"from sqlalchemy_utils import database_exists"
		]
	},
	"import snowpark": {
		"prefix": "import snowpark",
		"body": [
			"from snowflake.snowpark import Session",
			"from snowflake.snowpark.types import StructField, StructType, StringType, IntegerType"
		]
	},
	"import pandas.testing": {
		"prefix": "import pdtest",
		"body": [
			"import pandas.testing as pdtest"
		]
	},
	"import boto3": {
		"prefix": "import boto3",
		"body": [
			"from boto3.session import Session",
			"from botocore.response import StreamingBody",
			"from mypy_boto3_s3.service_resource import S3ServiceResource",
			"from mypy_boto3_ecs import ECSClient"
		]
	},

	// Logger

	"Logger":{
		"prefix":"logger_",
		"body":[
			"from logging import getLogger, Logger, Formatter, Handler, StreamHandler, DEBUG",
			"from pythonjsonlogger.jsonlogger import JsonFormatter",
			"default_logger: Logger = getLogger(__name__)",
			"formatter: Formatter = JsonFormatter(\"%(levelname)s %(module)s %(funcName)s %(message)s\")",
			"handler: Handler = StreamHandler()",
			"handler.setFormatter(formatter)",
			"default_logger.addHandler(handler)",
			"default_logger.setLevel(DEBUG)",
			"default_logger.propagate = False"
		]
	},
	"NullLogger": {
		"prefix": "logger_(null)",
		"body": [
			"from logging import getLogger, Logger, Formatter, Handler, NullHandler",
			"from pythonjsonlogger.jsonlogger import JsonFormatter",
			"null_logger: Logger = getLogger(__name__)",
			"formatter: Formatter = JsonFormatter(\"%(levelname)s %(module)s %(funcName)s %(message)s\")",
			"handler: Handler = NullHandler()",
			"handler.setFormatter(formatter)",
			"null_logger.addHandler(handler)",
			"null_logger.propagate = False"
		]
	},
	"StreamHandler": {
		"prefix": "handler_(stream)",
		"body": [
			"stream_handler: Handler = StreamHandler()",
			"stream_handler.setLevel(DEBUG)",
			"logger.addHandler(stream_handler)"
		]
	},
	"FlieHandler": {
		"prefix": "handler_(file)",
		"body": [
			"file_handler: Handler = FileHandler($0)",
			"file_handler.setLevel(DEBUG)",
			"logger.addHandler(file_handler)"
		]
	},
	"JsonFormatter": {
		"prefix": "formatter_(json)",
		"body": [
			"from pythonjsonlogger.jsonlogger import JsonFormatter",
			"formatter: Formatter = JsonFormatter(\"%(asctime)s %(levelname)s %(module)s %(funcName)s %(message)s\")"
		]
	},
	"warning(traceback)": {
		"prefix": "warning(traceback)",
		"body": [
			"logger.warning(traceback.format_exc())"
		]
	},

	// Utility

	"Entrypoint":{
		"prefix":["if __name__==\"__main__\"", "if_"],
		"body":[
			"if __name__ == \"__main__\":",
			"\t"
		]
	},
	"try/except": {
		"prefix": ["try/except", "try_", "try "],
		"body": [
			"try:",
			"\t${TM_SELECTED_TEXT}$0",
			"except Exception as e:",
			"\tlogger.warning(e)"
		]
	},
	"with open":{
		"prefix":["open_", "with open"],
		"body":[
			"with open(${TM_SELECTED_TEXT}$0, \"r\", encoding=\"utf-8\") as f:",
			"\t"
		]
	},
	"for filepath":{
		"prefix":["forfp", "for_fp"],
		"body":[
			"for filepath_$0 in tqdm(filepaths_$0):",
			"\tbreak"
		]
	},
	"for dict":{
		"prefix":["fordict"],
		"body":[
			"for key, val in tqdm(${TM_SELECTED_TEXT}$0.items()):",
			"\tbreak"
		]
	},
	"pathlib.Path": {
		"prefix": ["pathlib.Path", "pathlib_path", "path_"],
		"body": [
			"pathlib.Path(${TM_SELECTED_TEXT}$0)"
		]
	},
	"parent_dir": {
		"prefix": ["parent dir", "parent_"],
		"body": [
			"str(pathlib.Path(__file__).parents[0])"
		]
	},
	"grandparent_dir": {
		"prefix": ["grandparent dir", "grandparend_"],
		"body": [
			"str(pathlib.Path(__file__).parents[1])"
		]
	},
	"glob": {
		"prefix": ["glob.", "glob_", "gl_"],
		"body": [
			"glob.glob(f\"${TM_SELECTED_TEXT}$0/**/*\", recursive=True)"
		]
	},
	"str_format": {
		"prefix": "f",
		"body": [
			"f\"{${TM_SELECTED_TEXT}$0}\""
		]
	},
	"str_format(clipboard)": {
		"prefix": "fc",
		"body": [
			"f\"${CLIPBOARD}$0\""
		]
	},
	"re.search": {
		"prefix": "re_search",
		"body": [
			"re.search(\"[a-z]+\", ${TM_SELECTED_TEXT}$0).group()"
		]
	},
	"re.findall": {
		"prefix": "re_findall",
		"body": [
			"re.findall(\"[a-z]+\", ${TM_SELECTED_TEXT}$0)"
		]
	},
	"re.sub(format space)": {
		"prefix": "re_sub",
		"body": [
			"re.sub(\"\\s+\", \" \", ${TM_SELECTED_TEXT}$0)"
		]
	},
	"pbar": {
		"prefix": "pbar",
		"body": [
			"pbar = tqdm(${TM_SELECTED_TEXT}$0)"
		]
	},
	"pbar.set_postfix_str": {
		"prefix": ["pbar.set_postfix_str", "pbar_set_postfix_str"],
		"body": [
			"pbar.set_postfix_str(f\"$0\")"
		]
	},

	// TypeAlias
	"DictElement": {
		"prefix": "DictElement(TypeAlias)",
		"body": [
			"from typing import TypeAlias",
			"DictElement: TypeAlias = bool | int | float | str | datetime.date | None"
		]
	},

	// abbr

	"filepath_": {
		"prefix": "fp_",
		"body": [
			"filepath_$0"
		]
	},
	"filepaths_": {
		"prefix": "fps_",
		"body": [
			"filepaths_$0"
		]
	},
	"dirpath_": {
		"prefix": "dp_",
		"body": [
			"dirpath_$0"
		]
	},
	"dirpaths_": {
		"prefix": "dps_",
		"body": [
			"dirpaths_$0"
		]
	},
	"column": {
		"prefix": "col",
		"body": [
			"columns"
		]
	},
	"axis": {
		"prefix": "ax",
		"body": [
			"axis"
		]
	}
}
